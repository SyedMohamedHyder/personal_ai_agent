from config import MODEL, KNOWLEDGE_BASE, VECTOR_DB
from vectorstore.manager import inspect_vectorstore
from ui.chat_interface import launch_chat_interface
from core.pipeline import setup_environment, load_and_vectorize
from rag.conversation_chain import create_conversational_chain

# ───────────────────────────────────────────────────────────────────────────────
# ENV SETUP & PIPELINE RUN
# ───────────────────────────────────────────────────────────────────────────────

setup_environment()
documents, chunks, vectorstore, collection = load_and_vectorize(KNOWLEDGE_BASE, VECTOR_DB)

print(f"Total documents: {len(documents)}")
print(f"Total chunks: {len(chunks)}")
print(f"Document types: {set(doc.metadata['doc_type'] for doc in documents)}")

inspect_vectorstore(collection)

# ───────────────────────────────────────────────────────────────────────────────
# CONVERSATION CHAIN SETUP
# ───────────────────────────────────────────────────────────────────────────────

conversation_chain = create_conversational_chain(vectorstore, model_name=MODEL)

def chat(question, _):
    """
    Handle user questions by invoking the conversation chain.

    Args:
        question (str): The user’s question.
        _ (list): Unused chat history placeholder.

    Returns:
        str: Answer generated by the conversation chain.
    """
    result = conversation_chain.invoke({"question": question})
    return result["answer"]

# ───────────────────────────────────────────────────────────────────────────────
# LAUNCH CHAT UI
# ───────────────────────────────────────────────────────────────────────────────

launch_chat_interface(chat)
